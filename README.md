# ğŸ“Œ Kafka to Snowflake Streaming Pipeline  

## ğŸš€ Overview  
This project builds a **real-time data pipeline** that streams data from **Kafka**, processes it, and stores it in **Snowflake**.  

- **Producer** â†’ Generates and sends data to Kafka.  
- **Consumer** â†’ Reads, processes, and inserts data into Snowflake.  
- **Transformations** â†’ Cleans and formats the data.  
- **Configuration** â†’ Manages Kafka, Snowflake, and logging settings.  

---

## ğŸ“‚ Project Structure (Follow in Order)  

```
ğŸ“¦ Kafka-Snowflake-Pipeline
â”‚-- ğŸ“‚ requirements    # Dependencies & Setup Instructions (Start Here)
â”‚-- ğŸ“„ config.json     # Configuration File (Kafka, Snowflake, Logging)
â”‚-- ğŸ“‚ producer        # Kafka Producer - Generates & Sends Data
â”‚-- ğŸ“‚ transformations # Data Cleaning & Processing
â”‚-- ğŸ“‚ consumer        # Kafka Consumer - Reads & Inserts Data
â”‚-- ğŸ“„ README.md       # Project Documentation (You are here!)
```

---

## ğŸ”¹ 1ï¸âƒ£ Install Dependencies â†’ `requirements/`  
- This folder contains the required **Python libraries**.  
- Before running the project, install them using:  
  ```bash
  pip install -r requirements.txt
  ```

---

## ğŸ”¹ 2ï¸âƒ£ Configure the Pipeline â†’ `config.json`  
- Contains **Kafka, Snowflake, and Logging** settings.  
- **Modify credentials** before running.  

---

## ğŸ”¹ 3ï¸âƒ£ Start Kafka Producer â†’ `producer/producer.py`  
- Generates **random mock data**.  
- Sends it to **Kafka in real time**.  
- **Run this first before the consumer!**  
  ```bash
  python producer/producer.py
  ```

---

## ğŸ”¹ 4ï¸âƒ£ Handle Data Cleaning â†’ `transformations/transformations.py`  
- Cleans null values, special characters, and incorrect formats.  
- Standardizes **email, date, and numeric formats**.  
- This file is **automatically used by the consumer**, so no need to run it manually!  

---

## ğŸ”¹ 5ï¸âƒ£ Start Kafka Consumer â†’ `consumer/consumer.py`  
- Reads messages from Kafka.  
- Cleans & formats the data using **transformations**.  
- Inserts the data into **Snowflake** in batches.  
  ```bash
  python consumer/consumer.py
  ```

---

## ğŸ›‘ How to Stop the Pipeline  

1. **Stop the Producer** â†’ Press `CTRL + C` in the terminal.  
2. **Stop the Consumer** â†’ Press `CTRL + C` in the terminal.  
3. **Stop Kafka** â†’ Run:  
   ```bash
   kafka-server-stop.sh
   ```
4. **Stop Zookeeper** â†’ Run:  
   ```bash
   zookeeper-server-stop.sh
   ```
5. **If everything is still running, close VS Code** â†’ This force-stops all processes.  

---

## ğŸ¯ Next Steps  
âœ… **Test with real-time data sources** (APIs, logs, IoT).  
âœ… **Improve error handling** to handle failed inserts.  
âœ… **Optimize performance** by increasing batch sizes.  
âœ… **Automate deployment** using Docker/Kubernetes.  

---
